{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "this kernel use kaggle-real-and-fake-faces dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "import random\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential, load_model, save_model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications import * \n",
    "from tensorflow.keras.utils import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "# !pip install efficientnet -q\n",
    "import efficientnet.tfkeras as efn\n",
    "import gc\n",
    "import glob\n",
    "import itertools\n",
    "import cv2\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib_venn import venn2\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from tqdm.notebook import tqdm\n",
    "from termcolor import colored"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "40"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gc.collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Balance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "960"
      ]
     },
     "execution_count": 90,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_imgs = glob.glob('/Users/dph/downloads/data-real-and-fake-face-detection/training_fake/*.jpg')\n",
    "len(fake_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "960"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fake_y = np.ones(len(fake_imgs))\n",
    "len(fake_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "960"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_imgs = glob.glob('/Users/dph/downloads/data-real-and-fake-face-detection/training_real/*.jpg')[:len(fake_imgs)]\n",
    "len(real_imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "960"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "real_y = np.zeros(len(fake_imgs))\n",
    "len(real_y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1920"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "imgs = real_imgs + fake_imgs\n",
    "len(imgs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1920"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = np.hstack((real_y, fake_y))\n",
    "len(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "c = list(zip(imgs, y))\n",
    "random.shuffle(c)\n",
    "imgs, y = zip(*c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1152 614 154\n",
      "1152 614 154\n"
     ]
    }
   ],
   "source": [
    "train_imgs, test_imgs, train_y, test_y = train_test_split(imgs, y, test_size=0.4)\n",
    "val_imgs, test_imgs, val_y, test_y = train_test_split(test_imgs, test_y, test_size=0.2)\n",
    "print(len(train_imgs), len(val_imgs), len(test_imgs))\n",
    "print(len(train_y), len(val_y), len(test_y))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Agumentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b5ce6ab2ea994e27806278fe5ded030b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=192), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "tensor(0.4418) tensor(0.2753)\n"
     ]
    }
   ],
   "source": [
    "# get mean and std for all data\n",
    "class MeanDataset(Dataset):\n",
    "    def __init__(self, imgs):\n",
    "        self.imgs = imgs\n",
    "        \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.imgs[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (224, 224))/255.\n",
    "        return img\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "\n",
    "dataset = MeanDataset(imgs)\n",
    "loader = DataLoader(\n",
    "    dataset,\n",
    "    batch_size=10,\n",
    "    num_workers=0,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "\n",
    "mean = 0.\n",
    "std = 0.\n",
    "nb_samples = 0.\n",
    "for data in tqdm(loader, total=len(loader)):\n",
    "    batch_samples = data.size(0)\n",
    "    \n",
    "    mean += data.float().mean()\n",
    "    std += data.float().std()\n",
    "    nb_samples += 1\n",
    "\n",
    "mean /= nb_samples\n",
    "std /= nb_samples\n",
    "\n",
    "print(mean, std)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "from albumentations import Normalize, HorizontalFlip, Compose, RandomSizedCrop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_tf = Compose([HorizontalFlip(p=0.5), \n",
    "                    RandomSizedCrop(min_max_height=(180, 180), height=380, width=380, p=0.5),\n",
    "                    Normalize(mean=0.4418, std=0.2746)]) # this already includes /255.\n",
    "val_tf = Compose([Normalize(mean=0.4418, std=0.2746)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MyDataset(Dataset):\n",
    "    def __init__(self, imgs, y, split):\n",
    "        self.imgs = imgs\n",
    "        self.y = y\n",
    "        self.split = split\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.imgs)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        img_path = self.imgs[idx]\n",
    "        img = cv2.imread(img_path)\n",
    "        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "        img = cv2.resize(img, (380, 380))\n",
    "        \n",
    "        # augment\n",
    "        if self.split == 'train':\n",
    "            img = train_tf(**{'image': img})['image']\n",
    "        elif self.split == 'val':\n",
    "            img = val_tf(**{'image': img})['image']\n",
    "            \n",
    "        # \n",
    "        img = img.transpose(2, 0, 1)\n",
    "        \n",
    "        l = self.y[idx]\n",
    "        return img, l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_ds = MyDataset(train_imgs, train_y, 'train')\n",
    "train_dl = DataLoader(train_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "val_ds = MyDataset(val_imgs, val_y, 'val')\n",
    "val_dl = DataLoader(val_ds, batch_size=32, shuffle=False)\n",
    "\n",
    "test_ds = MyDataset(test_imgs, test_y, 'val')\n",
    "test_dl = DataLoader(test_ds, batch_size=32, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 380, 380]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(train_dl))\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 380, 380]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(val_dl))\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([32, 3, 380, 380]) torch.Size([32])\n"
     ]
    }
   ],
   "source": [
    "x, y = next(iter(test_dl))\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "from efficientnet_pytorch import EfficientNet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained weights for efficientnet-b4\n"
     ]
    }
   ],
   "source": [
    "class MyModel(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.base = EfficientNet.from_pretrained('efficientnet-b4')\n",
    "        self.fc = nn.Linear(self.base._fc.out_features, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.base(x)\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "model = MyModel() \n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze\n",
    "def freeze_until(model, layer):\n",
    "    flag = False\n",
    "    for n, p in model.named_parameters():\n",
    "        if n == layer:\n",
    "            flag = True\n",
    "        p.requires_grad = flag"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "# freeze_until(model, 'base._conv_head.weight')\n",
    "# for n, p in model.named_parameters():\n",
    "#     if p.requires_grad:\n",
    "#         print(n)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([2, 1])\n"
     ]
    }
   ],
   "source": [
    "x = torch.tensor(np.random.randn(2, 3, 224, 224)).float()\n",
    "y = model(x)\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.0001, weight_decay=0.)\n",
    "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=2, factor=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "prev = np.ones(len(train_dl))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_on(epoch, train_dl):\n",
    "    # prepare\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    model.train()\n",
    "    \n",
    "    loss_epoch = []\n",
    "    accuracy_epoch = []\n",
    "    for i, (x, y) in tqdm(enumerate(train_dl), total=len(train_dl)):\n",
    "        y_pred = model(x)\n",
    "        loss = F.binary_cross_entropy_with_logits(y_pred.squeeze(), y)\n",
    "        \n",
    "        if prev[i] == 1:\n",
    "            prev[i] = loss\n",
    "            print('Epoch: %d, Step: %d, Loss: %2f' % (epoch, i, loss))\n",
    "        else:\n",
    "            if loss < prev[i]:\n",
    "                print(colored('Epoch: %d, Step: %d, Loss: %2f' % (epoch, i, loss), 'green'))\n",
    "            else:\n",
    "                print(colored('Epoch: %d, Step: %d, Loss: %2f' % (epoch, i, loss), 'red'))\n",
    "                \n",
    "            prev[i] = loss\n",
    "            \n",
    "        \n",
    "        optimizer.zero_grad()\n",
    "        loss.backward() # calculate grad\n",
    "        optimizer.step() # update grad\n",
    "        \n",
    "        accuracy = (sum(torch.round(y) == torch.round(nn.Sigmoid()(y_pred.squeeze()))).float() / len(y)).item()\n",
    "        # add\n",
    "        loss_epoch.append(loss.item())\n",
    "        accuracy_epoch.append(accuracy)\n",
    "        \n",
    "        \n",
    "        del x, y, y_pred, loss, accuracy\n",
    "        \n",
    "    train_loss = sum(loss_epoch) / len(loss_epoch)\n",
    "    train_accuracy = sum(accuracy_epoch) / len(accuracy_epoch)\n",
    "\n",
    "    return train_loss, train_accuracy\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(epochs, train_dl):\n",
    "    for e in range(epochs):\n",
    "        train_on(e, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a2230dec5ce0433f9f339112e2816483",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=36), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mEpoch: 0, Step: 0, Loss: 0.629555\u001b[0m\n",
      "Epoch: 0, Step: 1, Loss: 0.737463\n",
      "Epoch: 0, Step: 2, Loss: 0.706230\n",
      "Epoch: 0, Step: 3, Loss: 0.740758\n",
      "Epoch: 0, Step: 4, Loss: 0.703336\n",
      "Epoch: 0, Step: 5, Loss: 0.712074\n",
      "Epoch: 0, Step: 6, Loss: 0.716560\n",
      "Epoch: 0, Step: 7, Loss: 0.702221\n",
      "Epoch: 0, Step: 8, Loss: 0.715054\n",
      "Epoch: 0, Step: 9, Loss: 0.724207\n",
      "Epoch: 0, Step: 10, Loss: 0.709788\n",
      "Epoch: 0, Step: 11, Loss: 0.726637\n",
      "Epoch: 0, Step: 12, Loss: 0.726076\n",
      "Epoch: 0, Step: 13, Loss: 0.673758\n",
      "Epoch: 0, Step: 14, Loss: 0.603374\n",
      "Epoch: 0, Step: 15, Loss: 0.737507\n",
      "Epoch: 0, Step: 16, Loss: 0.619352\n",
      "Epoch: 0, Step: 17, Loss: 0.617687\n",
      "Epoch: 0, Step: 18, Loss: 0.779100\n",
      "Epoch: 0, Step: 19, Loss: 0.654782\n",
      "Epoch: 0, Step: 20, Loss: 0.650066\n",
      "Epoch: 0, Step: 21, Loss: 0.650195\n",
      "Epoch: 0, Step: 22, Loss: 0.778417\n",
      "Epoch: 0, Step: 23, Loss: 0.695373\n",
      "Epoch: 0, Step: 24, Loss: 0.718566\n",
      "Epoch: 0, Step: 25, Loss: 0.615077\n",
      "Epoch: 0, Step: 26, Loss: 0.654718\n",
      "Epoch: 0, Step: 27, Loss: 0.678903\n",
      "Epoch: 0, Step: 28, Loss: 0.667219\n",
      "Epoch: 0, Step: 29, Loss: 0.687484\n",
      "Epoch: 0, Step: 30, Loss: 0.639557\n",
      "Epoch: 0, Step: 31, Loss: 0.718555\n",
      "Epoch: 0, Step: 32, Loss: 0.585100\n",
      "Epoch: 0, Step: 33, Loss: 0.690861\n",
      "Epoch: 0, Step: 34, Loss: 0.667686\n",
      "Epoch: 0, Step: 35, Loss: 0.585821\n",
      "\n"
     ]
    }
   ],
   "source": [
    "train(1, train_dl)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_prev = np.ones(len(val_dl))\n",
    "def test_on():\n",
    "    model.eval()\n",
    "    \n",
    "    loss_epoch = []\n",
    "    accuracy_epoch = []\n",
    "    \n",
    "    for i, (x, y) in tqdm(enumerate(val_dl), total=len(val_dl)):\n",
    "        with torch.no_grad():\n",
    "            x = x.to(device)\n",
    "            y = y.to(device)\n",
    "            y_preds = model(x).squeeze()\n",
    "            loss = F.binary_cross_entropy_with_logits(y_preds, y)\n",
    "            \n",
    "            if test_prev[i] == 1:\n",
    "                print('Step: %d, Test loss: %2f' % (i, loss))\n",
    "            else:\n",
    "                if loss < test_prev[i]:\n",
    "                    print(colored('Step: %d, Test loss: %2f' % (i, loss), 'green'))\n",
    "                else:\n",
    "                    print(colored('Step: %d, Test loss: %2f' % (i, loss), 'red'))\n",
    "            test_prev[i] = loss\n",
    "            \n",
    "            accuracy = (sum(torch.round(y) == torch.round(nn.Sigmoid()(y_pred))).float() / len(y)).item()\n",
    "            # add\n",
    "            loss_epoch.append(loss.item())\n",
    "            accuracy_epoch.append(accuracy)\n",
    "        \n",
    "            del x, y, y_pred, loss, accuracy\n",
    "    \n",
    "    val_loss = sum(loss_epoch)/len(loss_epoch)\n",
    "    val_accuracy = sum(accuracy_epoch) / len(accuracy_epoch)\n",
    "\n",
    "    return val_loss, val_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "12dbf997af6747bb83eea066760bc6ca",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=20), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[32mStep: 0, Test loss: 0.631987\u001b[0m\n",
      "\u001b[31mStep: 1, Test loss: 0.678470\u001b[0m\n",
      "\u001b[31mStep: 2, Test loss: 0.707027\u001b[0m\n",
      "\u001b[32mStep: 3, Test loss: 0.647057\u001b[0m\n",
      "\u001b[32mStep: 4, Test loss: 0.670705\u001b[0m\n",
      "\u001b[31mStep: 5, Test loss: 0.668898\u001b[0m\n",
      "\u001b[32mStep: 6, Test loss: 0.638163\u001b[0m\n",
      "\u001b[32mStep: 7, Test loss: 0.676597\u001b[0m\n",
      "\u001b[31mStep: 8, Test loss: 0.660896\u001b[0m\n",
      "\u001b[32mStep: 9, Test loss: 0.659189\u001b[0m\n",
      "\u001b[31mStep: 10, Test loss: 0.701660\u001b[0m\n",
      "\u001b[32mStep: 11, Test loss: 0.705386\u001b[0m\n",
      "\u001b[32mStep: 12, Test loss: 0.640857\u001b[0m\n",
      "\u001b[31mStep: 13, Test loss: 0.698955\u001b[0m\n",
      "\u001b[32mStep: 14, Test loss: 0.664973\u001b[0m\n",
      "\u001b[31mStep: 15, Test loss: 0.668699\u001b[0m\n",
      "\u001b[32mStep: 16, Test loss: 0.648419\u001b[0m\n",
      "\u001b[31mStep: 17, Test loss: 0.697917\u001b[0m\n",
      "\u001b[31mStep: 18, Test loss: 0.724641\u001b[0m\n",
      "\u001b[31mStep: 19, Test loss: 0.674494\u001b[0m\n",
      "\n"
     ]
    }
   ],
   "source": [
    "test_on()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
