{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os \n",
    "import matplotlib.pyplot as plt\n",
    "import gc\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Model, Sequential, load_model, save_model\n",
    "from tensorflow.keras.layers import *\n",
    "from tensorflow.keras.callbacks import TensorBoard, EarlyStopping, ReduceLROnPlateau\n",
    "from tensorflow.keras.applications import * \n",
    "from tensorflow.keras.utils import *\n",
    "from tensorflow.keras.optimizers import *\n",
    "\n",
    "# !pip install efficientnet -q\n",
    "import efficientnet.tfkeras as efn\n",
    "import gc\n",
    "import glob\n",
    "import itertools\n",
    "import cv2\n",
    "\n",
    "from sklearn.cluster import KMeans\n",
    "from matplotlib_venn import venn2\n",
    "\n",
    "import torch\n",
    "\n",
    "\n",
    "# retina face\n",
    "import sys\n",
    "sys.path.insert(0, '/Users/dph/downloads/Pytorch_Retinaface')\n",
    "from models.retinaface import RetinaFace\n",
    "from data import cfg_re50\n",
    "from layers.functions.prior_box import PriorBox\n",
    "from utils.box_utils import decode, decode_landm\n",
    "\n",
    "from skimage.transform import SimilarityTransform\n",
    "from PIL import Image"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Make Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch.autograd.grad_mode.set_grad_enabled at 0x1e9a3c7d0>"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.set_grad_enabled(False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "# because it is more diverse\n",
    "videos = glob.glob('/Users/dph/downloads/data-deepfake/dfdc_train_part_36/*.mp4')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2339"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [],
   "source": [
    "def norm_crop(img, landmark, image_size=112):\n",
    "    ARCFACE_SRC = np.array([[\n",
    "        [122.5, 141.25],\n",
    "        [197.5, 141.25],\n",
    "        [160.0, 178.75],\n",
    "        [137.5, 225.25],\n",
    "        [182.5, 225.25]\n",
    "    ]], dtype=np.float32)\n",
    "\n",
    "    def estimate_norm(lmk):\n",
    "        assert lmk.shape == (5, 2)\n",
    "\n",
    "        tform = SimilarityTransform()\n",
    "        lmk_tran = np.insert(lmk, 2, values=np.ones(5), axis=1)\n",
    "        min_M = []\n",
    "        min_index = []\n",
    "        min_error = np.inf\n",
    "        src = ARCFACE_SRC\n",
    "\n",
    "        for i in np.arange(src.shape[0]):\n",
    "            tform.estimate(lmk, src[i])\n",
    "        M = tform.params[0:2, :]\n",
    "\n",
    "        results = np.dot(M, lmk_tran.T)\n",
    "        results = results.T\n",
    "        error = np.sum(np.sqrt(np.sum((results - src[i]) ** 2, axis=1)))\n",
    "\n",
    "        if error < min_error:\n",
    "            min_error = error\n",
    "            min_M = M\n",
    "            min_index = i\n",
    "\n",
    "        return min_M, min_index\n",
    "\n",
    "    M, pose_index = estimate_norm(landmark)\n",
    "    warped = cv2.warpAffine(img, M, (image_size, image_size), borderValue=0.0)\n",
    "    return warped\n",
    "\n",
    "\n",
    "class FaceDetector:\n",
    "    def __init__(self, device=\"cpu\", confidence_threshold=0.8):\n",
    "        self.device = device\n",
    "        self.confidence_threshold = confidence_threshold\n",
    "\n",
    "        self.cfg = cfg_re50\n",
    "        self.variance = self.cfg[\"variance\"]\n",
    "        self.cfg[\"pretrain\"] = False\n",
    "\n",
    "        self.net = RetinaFace(cfg=self.cfg, phase=\"test\").to(device).eval()\n",
    "        self.decode_param_cache = {}\n",
    "\n",
    "    def load_checkpoint(self, path):\n",
    "        self.net.load_state_dict(torch.load(path, map_location=torch.device('cpu')))\n",
    "\n",
    "    def decode_params(self, height, width):\n",
    "        cache_key = (height, width)\n",
    "\n",
    "        try:\n",
    "            return self.decode_param_cache[cache_key]\n",
    "        except KeyError:\n",
    "            priorbox = PriorBox(self.cfg, image_size=(height, width))\n",
    "            priors = priorbox.forward()\n",
    "\n",
    "            prior_data = priors.data\n",
    "            scale = torch.Tensor([width, height] * 2)\n",
    "            scale1 = torch.Tensor([width, height] * 5)\n",
    "\n",
    "            result = (prior_data, scale, scale1)\n",
    "            self.decode_param_cache[cache_key] = result\n",
    "\n",
    "            return result\n",
    "\n",
    "    def detect(self, img):\n",
    "        device = self.device\n",
    "\n",
    "        prior_data, scale, scale1 = self.decode_params(*img.shape[:2])\n",
    "\n",
    "        # REF: test_fddb.py\n",
    "        img = np.float32(img)\n",
    "        img -= (104, 117, 123)\n",
    "        img = img.transpose(2, 0, 1)\n",
    "        img = torch.from_numpy(img).unsqueeze(0)\n",
    "        img = img.to(device, dtype=torch.float32)\n",
    "\n",
    "        loc, conf, landms = self.net(img)\n",
    "\n",
    "        loc = loc.cpu()\n",
    "        conf = conf.cpu()\n",
    "        landms = landms.cpu()\n",
    "\n",
    "        # Decode results\n",
    "        boxes = decode(loc.squeeze(0), prior_data, self.variance)\n",
    "        boxes = boxes * scale\n",
    "        scores = conf.squeeze(0)[:, 1]\n",
    "\n",
    "        landms = decode_landm(landms.squeeze(0), prior_data, self.variance)\n",
    "        landms = landms * scale1\n",
    "\n",
    "        inds = scores > self.confidence_threshold\n",
    "        boxes = boxes[inds]\n",
    "        landms = landms[inds]\n",
    "\n",
    "        return boxes, landms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "face_detector = FaceDetector()\n",
    "face_detector.load_checkpoint(\"/Users/dph/downloads/kaggle-dfdc/external/RetinaFace-Resnet50-fixed.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for v in videos:\n",
    "    reader = cv2.VideoCapture(v)\n",
    "    \n",
    "    for idx in itertools.count():\n",
    "        success, img = reader.read()\n",
    "        if not success:\n",
    "            break\n",
    "            \n",
    "        # boxes is face box\n",
    "        boxes, landms = face_detector.detect(img)\n",
    "        if boxes.shape[0] == 0:\n",
    "            continue\n",
    "            \n",
    "        # face area\n",
    "        areas = (boxes[:, 3] - boxes[:, 1]) * (boxes[:, 2] - boxes[:, 0])\n",
    "        max_face_idx = areas.argmax() # get the max face\n",
    "        landm = landms[max_face_idx]\n",
    "        \n",
    "        landmarks = landm.numpy().reshape(5, 2).astype(np.int)\n",
    "        img = norm_crop(img, landmarks, image_size=320)\n",
    "        aligned = Image.fromarray(img[:, :, ::-1])\n",
    "#         print(aligned)\n",
    "        \n",
    "        # save the img\n",
    "        out_dir = os.path.join('./outputs', v[35:-4])\n",
    "        if not os.path.exists(out_dir):\n",
    "            os.makedirs(out_dir)\n",
    "        out_path = os.path.join('./outputs', v[35:-4], \"%03d.jpg\" % idx)\n",
    "        aligned.save(out_path)   "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
