{"cells":[{"metadata":{},"cell_type":"markdown","source":"label smoothing"},{"metadata":{"_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","trusted":true},"cell_type":"code","source":"import os, sys, random\nimport numpy as np\nimport pandas as pd\nimport cv2\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import DataLoader\n\nfrom tqdm.notebook import tqdm\n\n%matplotlib inline\nimport matplotlib.pyplot as plt","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"gpu = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\ngpu","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"image_size = 224\nbatch_size = 64\ncv = 1\nLABELS = ['REAL', 'FAKE']\nlr = 0.01\nwd = 0.\npatience = 5\nfactor = 0.7","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Data"},{"metadata":{"_cell_guid":"","_uuid":"","trusted":true},"cell_type":"code","source":"crops_dir = \"../input/deepfake98493faces/outputs/\"\n\ndf = pd.read_csv(\"../input/deepfake98493faces/outputs/metadata.csv\")\ndf.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"len(df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"How many faces of each class do we have?"},{"metadata":{},"cell_type":"markdown","source":"Look at a random face image:"},{"metadata":{"trusted":true},"cell_type":"code","source":"img_path = os.path.join(crops_dir, np.random.choice(df.name_path.values))\nplt.imshow(cv2.imread(img_path)[..., ::-1])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Balance"},{"metadata":{"trusted":true},"cell_type":"code","source":"real_df = df[df.label == 'REAL']\nfake_df = df[df.label == 'FAKE']\nprint('Number of real is {}'.format(len(df[df.label == \"REAL\"])), 'Number of fake is {}'.format(len(df[df.label == \"FAKE\"])))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"real_df_2 = real_df.sample(frac=1)\nfake_df_2 = fake_df.sample(len(real_df_2))\n\nbalance_df = pd.concat([real_df_2, fake_df_2])\nprint(len(balance_df))","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Shuffle"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.utils import shuffle\nshuffle_df = shuffle(balance_df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Split\n60% train, 20% val, 20% test<br>"},{"metadata":{"trusted":true},"cell_type":"code","source":"from sklearn.model_selection import train_test_split\n\ntrain_df, val_df = train_test_split(shuffle_df, test_size=0.15)\n# val_df, test_df = train_test_split(test_df, test_size=0.5)\n\nprint(len(train_df), len(val_df))\nprint(round(len(train_df)/len(shuffle_df), 2), \n      round(len(val_df)/len(shuffle_df), 2)\n     )\nassert(len(train_df) + len(val_df) == len(shuffle_df))\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# len_cv = len(shuffle_df)//cv\n# dfs = []\n\n# for i in tqdm(range(cv)):  \n#     val_df = shuffle_df[i*len_cv : (i+1)*len_cv]\n#     train_df = shuffle_df.loc[~shuffle_df.index.isin(val_df.index)]\n#     print(len(train_df), len(val_df))\n    \n#     assert len(val_df) + len(train_df) == len(shuffle_df)\n    \n#     now_df = {\n#         'cv': i,\n#         'train_df': train_df,\n#         'val_df': val_df\n#     }\n#     dfs.append(now_df)\n    ","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"confirm validation sets has no intersection"},{"metadata":{"trusted":true},"cell_type":"code","source":"def val_intersections():\n    for i in range(cv):\n        for j in range(cv):\n            if i == j:\n                continue\n            else:\n                a = set(dfs[i]['val_df'].index.values).intersection(set(dfs[j]['val_df'].index.values))\n                if len(a) > 0:\n                    print('error', i, j)\n\nval_intersections()","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Augmentation"},{"metadata":{"trusted":true},"cell_type":"code","source":"from torchvision.transforms import Normalize\n\nclass Unnormalize:\n    \"\"\"Converts an image tensor that was previously Normalize'd\n    back to an image with pixels in the range [0, 1].\"\"\"\n    def __init__(self, mean, std):\n        self.mean = mean\n        self.std = std\n\n    def __call__(self, tensor):\n        mean = torch.as_tensor(self.mean, dtype=tensor.dtype, device=tensor.device).view(3, 1, 1)\n        std = torch.as_tensor(self.std, dtype=tensor.dtype, device=tensor.device).view(3, 1, 1)\n        return torch.clamp(tensor*std + mean, 0., 1.)\n\n\nmean = [0.485, 0.456, 0.406]\nstd = [0.229, 0.224, 0.225]\nnormalize = Normalize(mean, std)\nunnormalize = Unnormalize(mean, std)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def random_hflip(img, p=0.5):\n    \"\"\"Random horizontal flip.\"\"\"\n    if random.random() < p:\n        return cv2.flip(img, 1)\n    else:\n        return img","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import albumentations\nfrom albumentations.augmentations.transforms import ShiftScaleRotate, HorizontalFlip, Normalize, RandomBrightnessContrast, MotionBlur, Blur, GaussNoise, JpegCompression\ntrain_transform = albumentations.Compose([\n                                          ShiftScaleRotate(p=0.3, scale_limit=0.25, border_mode=1, rotate_limit=25),\n                                          HorizontalFlip(p=0.2),\n                                          RandomBrightnessContrast(p=0.3, brightness_limit=0.25, contrast_limit=0.5),\n                                          MotionBlur(p=.2),\n                                          GaussNoise(p=.2),\n                                          JpegCompression(p=.2, quality_lower=50),\n                                          Normalize()\n])\nval_transform = albumentations.Compose([Normalize()])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# GridMask"},{"metadata":{"trusted":true},"cell_type":"code","source":"# from albumentations.core.transforms_interface import DualTransform\n# from albumentations.augmentations import functional as F1\n\n# ## The author details are there along with the function\n# class GridMask(DualTransform):\n#     \"\"\"GridMask augmentation for image classification and object detection.\n    \n#     Author: Qishen Ha\n#     Email: haqishen@gmail.com\n#     2020/01/29\n\n#     Args:\n#         num_grid (int): number of grid in a row or column.\n#         fill_value (int, float, lisf of int, list of float): value for dropped pixels.\n#         rotate ((int, int) or int): range from which a random angle is picked. If rotate is a single int\n#             an angle is picked from (-rotate, rotate). Default: (-90, 90)\n#         mode (int):\n#             0 - cropout a quarter of the square of each grid (left top)\n#             1 - reserve a quarter of the square of each grid (left top)\n#             2 - cropout 2 quarter of the square of each grid (left top & right bottom)\n\n#     Targets:\n#         image, mask\n\n#     Image types:\n#         uint8, float32\n\n#     Reference:\n#     |  https://arxiv.org/abs/2001.04086\n#     |  https://github.com/akuxcw/GridMask\n#     \"\"\"\n\n#     def __init__(self, num_grid=3, fill_value=0, rotate=0, mode=0, always_apply=False, p=0.5):\n#         super(GridMask, self).__init__(always_apply, p)\n#         if isinstance(num_grid, int):\n#             num_grid = (num_grid, num_grid)\n#         if isinstance(rotate, int):\n#             rotate = (-rotate, rotate)\n#         self.num_grid = num_grid\n#         self.fill_value = fill_value\n#         self.rotate = rotate\n#         self.mode = mode\n#         self.masks = None\n#         self.rand_h_max = []\n#         self.rand_w_max = []\n\n#     def init_masks(self, height, width):\n#         if self.masks is None:\n#             self.masks = []\n#             n_masks = self.num_grid[1] - self.num_grid[0] + 1\n#             for n, n_g in enumerate(range(self.num_grid[0], self.num_grid[1] + 1, 1)):\n#                 grid_h = height / n_g\n#                 grid_w = width / n_g\n#                 this_mask = np.ones((int((n_g + 1) * grid_h), int((n_g + 1) * grid_w))).astype(np.uint8)\n#                 for i in range(n_g + 1):\n#                     for j in range(n_g + 1):\n#                         this_mask[\n#                              int(i * grid_h) : int(i * grid_h + grid_h / 2),\n#                              int(j * grid_w) : int(j * grid_w + grid_w / 2)\n#                         ] = self.fill_value\n#                         if self.mode == 2:\n#                             this_mask[\n#                                  int(i * grid_h + grid_h / 2) : int(i * grid_h + grid_h),\n#                                  int(j * grid_w + grid_w / 2) : int(j * grid_w + grid_w)\n#                             ] = self.fill_value\n                \n#                 if self.mode == 1:\n#                     this_mask = 1 - this_mask\n\n#                 self.masks.append(this_mask)\n#                 self.rand_h_max.append(grid_h)\n#                 self.rand_w_max.append(grid_w)\n\n#     def apply(self, image, mask, rand_h, rand_w, angle, **params):\n#         h, w = image.shape[:2]\n#         mask = F1.rotate(mask, angle) if self.rotate[1] > 0 else mask\n#         mask = mask[:,:,np.newaxis] if image.ndim == 3 else mask\n#         image *= mask[rand_h:rand_h+h, rand_w:rand_w+w].astype(image.dtype)\n#         return image\n\n#     def get_params_dependent_on_targets(self, params):\n#         img = params['image']\n#         height, width = img.shape[:2]\n#         self.init_masks(height, width)\n\n#         mid = np.random.randint(len(self.masks))\n#         mask = self.masks[mid]\n#         rand_h = np.random.randint(self.rand_h_max[mid])\n#         rand_w = np.random.randint(self.rand_w_max[mid])\n#         angle = np.random.randint(self.rotate[0], self.rotate[1]) if self.rotate[1] > 0 else 0\n\n#         return {'mask': mask, 'rand_h': rand_h, 'rand_w': rand_w, 'angle': angle}\n\n#     @property\n#     def targets_as_params(self):\n#         return ['image']\n\n#     def get_transform_init_args_names(self):\n#         return ('num_grid', 'fill_value', 'rotate', 'mode')\n    \n# train_transform = albumentations.Compose([\n#                                           ShiftScaleRotate(p=0.3, scale_limit=0.25, border_mode=1),\n#                                           HorizontalFlip(p=0.2),\n#                                           albumentations.RandomCrop(150,150),\n#                                           albumentations.OneOf([\n#                                             GridMask(num_grid=(1,3),rotate=15),\n#                                             GridMask(num_grid=(2,4), mode=0),\n#                                             GridMask(num_grid=3, mode=2),\n#                                           ], p=0.5),\n#                                           Normalize()\n# ])\n# val_transform = albumentations.Compose([\n#                                         albumentations.RandomCrop(150,150),  \n#                                         Normalize()\n# ])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# effect of augmentation\n\nimg_path = os.path.join(crops_dir, np.random.choice(df.name_path.values))\nimg1 = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\nimg2 = val_transform(**{'image':img1})['image']\n\nfig, ax = plt.subplots(2, 3, figsize=(8, 8))\nax[0, 0].imshow(img1)\nax[0, 1].imshow(img2)\nax[0, 2].imshow(unnormalize(torch.tensor(img2).permute(2, 0, 1)).permute(1, 2, 0))\n\n\nimg_path = os.path.join(crops_dir, np.random.choice(df.name_path.values))\nimg1 = cv2.cvtColor(cv2.imread(img_path), cv2.COLOR_BGR2RGB)\nimg2 = train_transform(**{'image':img1})['image']\n\nax[1, 0].imshow(img1)\nax[1, 1].imshow(img2)\nax[1, 2].imshow(unnormalize(torch.tensor(img2).permute(2, 0, 1)).permute(1, 2, 0))","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"from torch.utils.data import Dataset\n\nclass VideoDataset(Dataset):\n\n    def __init__(self, df, split, augment=True):\n        self.crops_dir = crops_dir\n        self.split = split\n        self.augment = augment\n        self.df = df\n        \n        num_real = len(self.df[self.df[\"label\"] == \"REAL\"])\n        num_fake = len(self.df[self.df[\"label\"] == \"FAKE\"])\n        print(\"%s dataset has %d real videos, %d fake videos\" % (split, num_real, num_fake))\n  \n    def __getitem__(self, index):\n        row = self.df.iloc[index]\n        filename = row[\"name_path\"][:-4] + \".jpg\"\n        cls = row[\"label\"]\n        \n        # get img\n        img = cv2.imread(os.path.join(self.crops_dir, row[\"name_path\"]))\n        img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n        \n        # augmentation\n        if self.split == 'train' and self.augment:\n            img = train_transform(**{'image': img})['image']\n        elif self.split == 'val' and self.augment:\n            img = val_transform(**{'image': img})['image']\n        img = torch.tensor(img).permute((2, 0, 1)).float()\n        \n        # get label\n        target = LABELS.index(row['label'])\n        \n        return img, target\n    \n    \n    \n    def __len__(self):\n        return len(self.df)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Let's test that the dataset actually works..."},{"metadata":{"trusted":true},"cell_type":"code","source":"dataset = VideoDataset(val_df, \"val\")\nplt.imshow(unnormalize(dataset[0][0]).permute(1, 2, 0))\ndel dataset","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"train_dataset = VideoDataset(train_df, \"train\", True)\ntrain_dl = DataLoader(train_dataset, batch_size=batch_size, shuffle=True,\n                      num_workers=4, pin_memory=True)\n\nval_dataset = VideoDataset(val_df, \"val\", True)\nval_dl = DataLoader(val_dataset, batch_size=batch_size, shuffle=False,\n                    num_workers=0, pin_memory=True)\n\ntest_dataset = VideoDataset(test_df, \"val\", True)\ntest_dl = DataLoader(test_dataset, batch_size=batch_size, shuffle=False,\n                    num_workers=0, pin_memory=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = next(iter(train_dl))\nplt.imshow(unnormalize(X[0]).permute(1, 2, 0))\nprint(y[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = next(iter(val_dl))\nplt.imshow(unnormalize(X[0]).permute(1, 2, 0))\nprint(y[0])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"X, y = next(iter(test_dl))\nplt.imshow(unnormalize(X[0]).permute(1, 2, 0))\nprint(y[0])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Model"},{"metadata":{"trusted":true},"cell_type":"code","source":"checkpoint = torch.load(\"../input/pretrained-pytorch/resnext50_32x4d-7cdf4587.pth\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"import torchvision.models as models\n\nclass MyResNeXt(models.resnet.ResNet):\n    def __init__(self, training=True):\n        super(MyResNeXt, self).__init__(block=models.resnet.Bottleneck,\n                                        layers=[3, 4, 6, 3], \n                                        groups=32, \n                                        width_per_group=4)\n\n        self.load_state_dict(checkpoint)\n\n        # Override the existing FC layer with a new one.\n        self.fc = nn.Linear(2048, 1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"net = MyResNeXt().to(gpu)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"del checkpoint","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Test the model on a small batch to see what its output shape is:"},{"metadata":{"trusted":true},"cell_type":"code","source":"out = net(torch.zeros((10, 3, 224, 224)).to(gpu))\nout.shape","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Freeze the early layers of the model:"},{"metadata":{"trusted":true},"cell_type":"code","source":"def freeze_until(net, param_name):\n    found_name = False\n    for name, params in net.named_parameters():\n        if name == param_name:\n            found_name = True\n        params.requires_grad = found_name","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"freeze_until(net, \"layer4.0.conv1.weight\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"These are the layers we will train:"},{"metadata":{"trusted":true},"cell_type":"code","source":"[k for k,v in net.named_parameters() if v.requires_grad]","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"# Training & Evaluating"},{"metadata":{"trusted":true},"cell_type":"code","source":"history = { \"train_bce\": [], \"val_bce\": [] }\niteration = 0\nepochs_done = 0\n\noptimizer = torch.optim.Adam(net.parameters(), lr=lr, weight_decay=wd)\nscheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, patience=patience, mode='min', factor=factor, verbose=True, min_lr=1e-5)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"def evaluate(net, data_loader, device, silent=False):\n    net.train(False)\n\n    bce_loss = 0\n    total_examples = 0\n\n    with tqdm(total=len(data_loader), desc=\"Evaluation\", leave=False, disable=silent) as pbar:\n        for batch_idx, data in enumerate(data_loader):\n            with torch.no_grad():\n                batch_size = data[0].shape[0]\n                x = data[0].to(device)\n                y_true = data[1].to(device).float()\n\n                y_pred = net(x)\n                y_pred = y_pred.squeeze()\n\n                bce_loss += F.binary_cross_entropy_with_logits(y_pred, y_true).item() * batch_size\n\n            total_examples += batch_size\n            pbar.update()\n\n    bce_loss /= total_examples\n\n    if silent:\n        return bce_loss\n    else:\n        print(\"BCE: %.4f\" % (bce_loss))\n        \n    if scheduler is not None:\n        scheduler.step(bce_loss)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"evaluate(net, val_dl, device=gpu)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"## Training"},{"metadata":{"trusted":true},"cell_type":"code","source":"def fit(epochs, train_dl, val_dl, net, optimizer):\n    global history, iteration, epochs_done, lr\n\n    with tqdm(total=len(train_dl), leave=False) as pbar:\n        for epoch in range(epochs):\n            pbar.reset()\n            pbar.set_description(\"Epoch %d\" % (epochs_done + 1))\n            \n            bce_loss = 0\n            total_examples = 0\n\n            net.train(True)\n\n            for batch_idx, data in enumerate(train_dl):\n                batch_size = data[0].shape[0]\n                x = data[0].to(gpu)\n                y_true = data[1].to(gpu).float()\n                \n                optimizer.zero_grad()\n\n                y_pred = net(x)\n                y_pred = y_pred.squeeze()\n                \n                loss = F.binary_cross_entropy_with_logits(y_pred, y_true)\n                loss.backward()\n                optimizer.step()\n                \n                batch_bce = loss.item()\n                bce_loss += batch_bce * batch_size\n                history[\"train_bce\"].append(batch_bce)\n\n                total_examples += batch_size\n                iteration += 1\n                pbar.update()\n\n            bce_loss /= total_examples\n            epochs_done += 1\n\n            print(\"Epoch: %3d, train BCE: %.4f\" % (epochs_done, bce_loss))\n\n            val_bce_loss = evaluate(net, val_dl, device=gpu, silent=True)\n            history[\"val_bce\"].append(val_bce_loss)\n            \n            print(\"              val BCE: %.4f\" % (val_bce_loss))\n\n            # TODO: can do LR annealing here\n            # TODO: can save checkpoint here\n            # save the best model\n            if round(val_bce_loss, 4) <= round(min(history['val_bce']), 4):\n                print('save the model')\n                ck = {'epoch': epochs_done, \n                     'state_dict': net.state_dict(),\n                     'optimizer': optimizer.state_dict(),\n                     'val_loss': round(val_bce_loss, 4)}\n                path = 'best_model_.pth'\n                torch.save(ck, path)\n                \n\n            print(\"\")","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"At this point you can load the model from the previous checkpoint. If you do, also make sure to restore the optimizer state! Something like this:\n\n```python\ncheckpoint = torch.load(\"model-checkpoint.pth\")\nnet.load_state_dict(checkpoint)\n\ncheckpoint = torch.load(\"optimizer-checkpoint.pth\")\noptimizer.load_state_dict(checkpoint)\n```"},{"metadata":{},"cell_type":"markdown","source":"Let's start training!"},{"metadata":{"trusted":true},"cell_type":"code","source":"def set_lr(optimizer, lr):\n    for param_group in optimizer.param_groups:\n        param_group[\"lr\"] = lr","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"set_lr(optimizer, lr)\nfit(5, train_dl, val_dl, net, optimizer)","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"Plot training progress. It's nicer to use something like TensorBoard for this, but a simple plot also works. ;-)"},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history[\"train_bce\"])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"plt.plot(history[\"val_bce\"])","execution_count":null,"outputs":[]},{"metadata":{},"cell_type":"markdown","source":"**All done!** You can now use this checkpoint in the [inference kernel](https://www.kaggle.com/humananalog/inference-demo)."},{"metadata":{},"cell_type":"markdown","source":"# Test"},{"metadata":{"trusted":true},"cell_type":"code","source":"# final model\nevaluate(net, test_dl, device=gpu)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# reset to best model\nck = torch.load('best_model_.pth')\nnet.load_state_dict(ck['state_dict'])\noptimizer.load_state_dict(ck['optimizer'])","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# best model \nevaluate(net, test_dl, device=gpu)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true},"cell_type":"code","source":"# dwonload the file\nfrom IPython.display import FileLink\nFileLink('best_model_.pth') ","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.6.6"},"widgets":{"application/vnd.jupyter.widget-state+json":{"state":{"09e9a18ff5b845b283677c263c011941":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"1dd0ee6d24a947b78efb275a3c0a6108":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"Evaluation: 100%","description_tooltip":null,"layout":"IPY_MODEL_9ffc3fdb423b41a090c2b6a4f35e5935","max":16,"min":0,"orientation":"horizontal","style":"IPY_MODEL_28d7220d3dff4ab5b4916bd9d40a1bc5","value":16}},"28d7220d3dff4ab5b4916bd9d40a1bc5":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"2d1cd44b25d44438a29b38c2c3e09f3e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_30cdfc08ee49486ba313e38f96a31b4c","IPY_MODEL_5de9e98309454a829444c8a7569d60f7"],"layout":"IPY_MODEL_c853445fb2684409906685c1ff2224ff"}},"2ef07498f9aa43a499614d5422f45032":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_3b65a690448e41f8b2acf3ddc6dff3bf","placeholder":"​","style":"IPY_MODEL_e72f1a5dfc3f401eb18cfe5d113d86d9","value":" 313/313 [01:15&lt;00:00,  5.06it/s]"}},"30cdfc08ee49486ba313e38f96a31b4c":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"Epoch 5: 100%","description_tooltip":null,"layout":"IPY_MODEL_09e9a18ff5b845b283677c263c011941","max":313,"min":0,"orientation":"horizontal","style":"IPY_MODEL_a0bb15965c2447c58dca32abe4bb5fa7","value":313}},"3b65a690448e41f8b2acf3ddc6dff3bf":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"4010ae9eb32f4006a17c2594f2474004":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"5de9e98309454a829444c8a7569d60f7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_cd34feac7a74414190aef7c074d08cd9","placeholder":"​","style":"IPY_MODEL_ebe147d3cba242a78952a88ef76edd17","value":" 313/313 [01:14&lt;00:00,  5.15it/s]"}},"78d0b713e3c64950b7c227faf83e7086":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"8967e0f4ce744ce6b0bf08fbb2f8b70e":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"8eef86e1ffcb433d9891c21697c1bb51":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"IntProgressModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"IntProgressModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"ProgressView","bar_style":"","description":"Epoch 10: 100%","description_tooltip":null,"layout":"IPY_MODEL_d36e152f7e8e48cd9262daa7dfb05c9c","max":313,"min":0,"orientation":"horizontal","style":"IPY_MODEL_78d0b713e3c64950b7c227faf83e7086","value":313}},"9ffc3fdb423b41a090c2b6a4f35e5935":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"a0bb15965c2447c58dca32abe4bb5fa7":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"ProgressStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"ProgressStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","bar_color":null,"description_width":"initial"}},"a2234b7ac1f24734a491760baa8edf67":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c04eee7e5cf441f8919dc51b6bef8007":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"c853445fb2684409906685c1ff2224ff":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"cafe5b82724949a29449bcf954fdb209":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HTMLModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HTMLModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HTMLView","description":"","description_tooltip":null,"layout":"IPY_MODEL_c04eee7e5cf441f8919dc51b6bef8007","placeholder":"​","style":"IPY_MODEL_8967e0f4ce744ce6b0bf08fbb2f8b70e","value":" 16/16 [00:04&lt;00:00,  4.73it/s]"}},"cd32ff90db7d45a6b09ddcc5ea349501":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_8eef86e1ffcb433d9891c21697c1bb51","IPY_MODEL_2ef07498f9aa43a499614d5422f45032"],"layout":"IPY_MODEL_a2234b7ac1f24734a491760baa8edf67"}},"cd34feac7a74414190aef7c074d08cd9":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d36e152f7e8e48cd9262daa7dfb05c9c":{"model_module":"@jupyter-widgets/base","model_module_version":"1.2.0","model_name":"LayoutModel","state":{"_model_module":"@jupyter-widgets/base","_model_module_version":"1.2.0","_model_name":"LayoutModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"LayoutView","align_content":null,"align_items":null,"align_self":null,"border":null,"bottom":null,"display":null,"flex":null,"flex_flow":null,"grid_area":null,"grid_auto_columns":null,"grid_auto_flow":null,"grid_auto_rows":null,"grid_column":null,"grid_gap":null,"grid_row":null,"grid_template_areas":null,"grid_template_columns":null,"grid_template_rows":null,"height":null,"justify_content":null,"justify_items":null,"left":null,"margin":null,"max_height":null,"max_width":null,"min_height":null,"min_width":null,"object_fit":null,"object_position":null,"order":null,"overflow":null,"overflow_x":null,"overflow_y":null,"padding":null,"right":null,"top":null,"visibility":null,"width":null}},"d6177027cd1642c897c817de87979342":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"HBoxModel","state":{"_dom_classes":[],"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"HBoxModel","_view_count":null,"_view_module":"@jupyter-widgets/controls","_view_module_version":"1.5.0","_view_name":"HBoxView","box_style":"","children":["IPY_MODEL_1dd0ee6d24a947b78efb275a3c0a6108","IPY_MODEL_cafe5b82724949a29449bcf954fdb209"],"layout":"IPY_MODEL_4010ae9eb32f4006a17c2594f2474004"}},"e72f1a5dfc3f401eb18cfe5d113d86d9":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}},"ebe147d3cba242a78952a88ef76edd17":{"model_module":"@jupyter-widgets/controls","model_module_version":"1.5.0","model_name":"DescriptionStyleModel","state":{"_model_module":"@jupyter-widgets/controls","_model_module_version":"1.5.0","_model_name":"DescriptionStyleModel","_view_count":null,"_view_module":"@jupyter-widgets/base","_view_module_version":"1.2.0","_view_name":"StyleView","description_width":""}}},"version_major":2,"version_minor":0}}},"nbformat":4,"nbformat_minor":4}